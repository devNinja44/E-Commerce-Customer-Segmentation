{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "## 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters for KMeans\n",
    "print(\"Finding optimal number of clusters for KMeans...\")\n",
    "k_range = range(2, 11)\n",
    "optimal_k_elbow, optimal_k_silhouette, inertia_values, silhouette_values = find_optimal_k(\n",
    "    X_reduced, \n",
    "    k_range=k_range, \n",
    "    random_state=config['clustering']['kmeans']['random_state']\n",
    ")\n",
    "\n",
    "print(f\"Optimal k based on elbow method: {optimal_k_elbow}\")\n",
    "print(f\"Optimal k based on silhouette score: {optimal_k_silhouette}\")\n",
    "\n",
    "# Plot elbow method results\n",
    "fig = plot_elbow_method(k_range, inertia_values, silhouette_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering using the optimal number of clusters\n",
    "print(f\"Applying {config['clustering']['method']} clustering...\")\n",
    "\n",
    "# Update config with optimal number of clusters if using KMeans\n",
    "if config['clustering']['method'] == 'kmeans':\n",
    "    # Choose the optimal k based on silhouette score\n",
    "    config['clustering']['kmeans']['n_clusters'] = optimal_k_silhouette\n",
    "    print(f\"Using optimal number of clusters: {optimal_k_silhouette}\")\n",
    "\n",
    "# Get method-specific parameters\n",
    "method = config['clustering']['method']\n",
    "method_params = config['clustering'][method]\n",
    "\n",
    "# Apply clustering\n",
    "labels, model, metrics = cluster_data(\n",
    "    X_reduced,\n",
    "    method=method,\n",
    "    **method_params\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nClustering metrics:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"- {metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "print(\"Visualizing clustering results...\")\n",
    "\n",
    "# Get centroids if using KMeans\n",
    "centroids = None\n",
    "if method == 'kmeans':\n",
    "    centroids = model.cluster_centers_\n",
    "\n",
    "# Plot clusters\n",
    "fig = plot_clusters_2d(\n",
    "    X_reduced, \n",
    "    labels, \n",
    "    centroids=centroids, \n",
    "    title=f'{method.upper()} Clustering Results',\n",
    "    figsize=config['visualization']['figsize'],\n",
    "    alpha=config['visualization']['alpha'],\n",
    "    s=config['visualization']['s']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different clustering methods\n",
    "print(\"Comparing different clustering methods...\")\n",
    "\n",
    "# Define methods to compare\n",
    "clustering_methods = ['kmeans', 'dbscan', 'agglomerative']\n",
    "\n",
    "# Define parameters for each method\n",
    "clustering_params = {\n",
    "    'kmeans': {\n",
    "        'n_clusters': optimal_k_silhouette,\n",
    "        'random_state': config['clustering']['kmeans']['random_state']\n",
    "    },\n",
    "    'dbscan': {\n",
    "        'eps': config['clustering']['dbscan']['eps'],\n",
    "        'min_samples': config['clustering']['dbscan']['min_samples']\n",
    "    },\n",
    "    'agglomerative': {\n",
    "        'n_clusters': optimal_k_silhouette,\n",
    "        'linkage': config['clustering']['agglomerative']['linkage']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply each method\n",
    "clustering_results = compare_clustering_methods(\n",
    "    X_reduced,\n",
    "    methods=clustering_methods,\n",
    "    **clustering_params\n",
    ")\n",
    "\n",
    "# Extract labels for visualization\n",
    "labels_dict = {}\n",
    "for method_name, (labels_method, _, _) in clustering_results.items():\n",
    "    labels_dict[method_name.upper()] = labels_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results for each method\n",
    "from src.visualization import compare_dimensionality_reduction_methods as compare_vis\n",
    "\n",
    "# Create a dictionary with the same reduced data for each method\n",
    "X_dict_for_vis = {method_name.upper(): X_reduced for method_name in clustering_methods}\n",
    "\n",
    "# Plot clustering results\n",
    "fig = compare_vis(X_dict_for_vis, labels_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## 6. Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering results using evaluation metrics\n",
    "print(\"Comparing clustering results using evaluation metrics...\")\n",
    "comparison_df = compare_clustering_results(X_reduced, clustering_results)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best clustering method based on silhouette score\n",
    "best_method = comparison_df['silhouette'].idxmax()\n",
    "print(f\"Best clustering method based on silhouette score: {best_method}\")\n",
    "\n",
    "# Get the labels from the best method\n",
    "best_labels = clustering_results[best_method.lower()][0]\n",
    "\n",
    "# Create an interactive visualization of the best clustering result\n",
    "if customer_ids is not None:\n",
    "    hover_data = pd.DataFrame({'customer_id': customer_ids})\n",
    "else:\n",
    "    hover_data = None\n",
    "\n",
    "fig = create_interactive_scatter(\n",
    "    X_reduced, \n",
    "    best_labels, \n",
    "    hover_data=hover_data, \n",
    "    title=f'{best_method} Clustering Results'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "## 7. Cluster Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "print(\"Analyzing clusters...\")\n",
    "\n",
    "# Get original feature names\n",
    "feature_names = df.columns.tolist()\n",
    "\n",
    "# Analyze clusters using the best labels\n",
    "cluster_profiles = analyze_clusters(df.values, best_labels, feature_names)\n",
    "cluster_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster profiles\n",
    "fig = plot_cluster_profiles(cluster_profiles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate human-readable labels for clusters\n",
    "cluster_labels = generate_cluster_labels(cluster_profiles)\n",
    "\n",
    "print(\"Cluster labels:\")\n",
    "for cluster_id, label in cluster_labels.items():\n",
    "    size = cluster_profiles.loc[cluster_id, 'Size'] if 'Size' in cluster_profiles.columns else 'N/A'\n",
    "    print(f\"Cluster {cluster_id} ({size} customers): {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with customer IDs and cluster labels\n",
    "if customer_ids is not None:\n",
    "    customer_clusters = pd.DataFrame({\n",
    "        'customer_id': customer_ids,\n",
    "        'cluster': best_labels,\n",
    "        'cluster_name': [cluster_labels[label] for label in best_labels]\n",
    "    })\n",
    "    customer_clusters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='edge_cases'></a>\n",
    "## 8. Edge Cases and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness with small dataset\n",
    "print(\"Testing robustness with small dataset...\")\n",
    "\n",
    "# Generate a small dataset\n",
    "    # Load a small subset of the real data\n",
    "    df_full, _ = load_online_retail_data(\"None\")\n",
    "    small_df = df_full.sample(n=30, random_state=42)\n",
    "print(f\"Small dataset shape: {small_df.shape}\")\n",
    "\n",
    "# Preprocess the small dataset\n",
    "small_df_processed = preprocess_data(small_df.drop('customer_id', axis=1), config=config['preprocessing'])\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "small_X_reduced, _, _ = reduce_dimensions(\n",
    "    small_df_processed.values,\n",
    "    method=config['dimensionality_reduction']['method'],\n",
    "    n_components=config['dimensionality_reduction']['n_components'],\n",
    "    random_state=config['dimensionality_reduction']['random_state'],\n",
    "    **config['dimensionality_reduction'][config['dimensionality_reduction']['method']]\n",
    ")\n",
    "\n",
    "# Apply clustering\n",
    "small_labels, small_model, small_metrics = cluster_data(\n",
    "    small_X_reduced,\n",
    "    method=config['clustering']['method'],\n",
    "    **config['clustering'][config['clustering']['method']]\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nClustering metrics for small dataset:\")\n",
    "for metric_name, metric_value in small_metrics.items():\n",
    "    print(f\"- {metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "fig = plot_clusters_2d(\n",
    "    small_X_reduced, \n",
    "    small_labels, \n",
    "    title=f'{config[\"clustering\"][\"method\"].upper()} Clustering Results (Small Dataset)',\n",
    "    figsize=config['visualization']['figsize'],\n",
    "    alpha=config['visualization']['alpha'],\n",
    "    s=config['visualization']['s']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness with high-dimensional noisy data\n",
    "print(\"Testing robustness with high-dimensional noisy data...\")\n",
    "\n",
    "# Generate a dataset with additional noisy features\n",
    "    # Load data and add noise\n",
    "    df_full, _ = load_online_retail_data(\"None\")\n",
    "    noisy_df = df_full.sample(n=200, random_state=42).copy()\n",
    "    # Add noise to numerical columns\n",
    "    for col in noisy_df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "        noise = np.random.normal(0, noisy_df[col].std() * 0.2, size=len(noisy_df))\n",
    "        noisy_df[col] = noisy_df[col] + noise\n",
    "\n",
    "# Add noisy features\n",
    "for i in range(10):\n",
    "    noisy_df[f'noise_{i}'] = np.random.normal(0, 1, size=len(noisy_df))\n",
    "\n",
    "print(f\"Noisy dataset shape: {noisy_df.shape}\")\n",
    "\n",
    "# Preprocess the noisy dataset\n",
    "noisy_df_processed = preprocess_data(noisy_df.drop('customer_id', axis=1), config=config['preprocessing'])\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "noisy_X_reduced, _, _ = reduce_dimensions(\n",
    "    noisy_df_processed.values,\n",
    "    method=config['dimensionality_reduction']['method'],\n",
    "    n_components=config['dimensionality_reduction']['n_components'],\n",
    "    random_state=config['dimensionality_reduction']['random_state'],\n",
    "    **config['dimensionality_reduction'][config['dimensionality_reduction']['method']]\n",
    ")\n",
    "\n",
    "# Apply clustering\n",
    "noisy_labels, noisy_model, noisy_metrics = cluster_data(\n",
    "    noisy_X_reduced,\n",
    "    method=config['clustering']['method'],\n",
    "    **config['clustering'][config['clustering']['method']]\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nClustering metrics for noisy dataset:\")\n",
    "for metric_name, metric_value in noisy_metrics.items():\n",
    "    print(f\"- {metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "fig = plot_clusters_2d(\n",
    "    noisy_X_reduced, \n",
    "    noisy_labels, \n",
    "    title=f'{config[\"clustering\"][\"method\"].upper()} Clustering Results (Noisy Dataset)',\n",
    "    figsize=config['visualization']['figsize'],\n",
    "    alpha=config['visualization']['alpha'],\n",
    "    s=config['visualization']['s']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness with imbalanced clusters\n",
    "print(\"Testing robustness with imbalanced clusters...\")\n",
    "\n",
    "# Generate a dataset with imbalanced clusters\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate cluster 1 (80% of data)\n",
    "cluster1_size = int(0.8 * n_samples)\n",
    "cluster1_data = np.random.normal(0, 1, size=(cluster1_size, 2))\n",
    "\n",
    "# Generate cluster 2 (15% of data)\n",
    "cluster2_size = int(0.15 * n_samples)\n",
    "cluster2_data = np.random.normal(5, 1, size=(cluster2_size, 2))\n",
    "\n",
    "# Generate cluster 3 (5% of data)\n",
    "cluster3_size = n_samples - cluster1_size - cluster2_size\n",
    "cluster3_data = np.random.normal(-5, 1, size=(cluster3_size, 2))\n",
    "\n",
    "# Combine clusters\n",
    "imbalanced_data = np.vstack([cluster1_data, cluster2_data, cluster3_data])\n",
    "print(f\"Imbalanced dataset shape: {imbalanced_data.shape}\")\n",
    "\n",
    "# Apply clustering\n",
    "imbalanced_labels, imbalanced_model, imbalanced_metrics = cluster_data(\n",
    "    imbalanced_data,\n",
    "    method=config['clustering']['method'],\n",
    "    **config['clustering'][config['clustering']['method']]\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nClustering metrics for imbalanced dataset:\")\n",
    "for metric_name, metric_value in imbalanced_metrics.items():\n",
    "    print(f\"- {metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "fig = plot_clusters_2d(\n",
    "    imbalanced_data, \n",
    "    imbalanced_labels, \n",
    "    title=f'{config[\"clustering\"][\"method\"].upper()} Clustering Results (Imbalanced Dataset)',\n",
    "    figsize=config['visualization']['figsize'],\n",
    "    alpha=config['visualization']['alpha'],\n",
    "    s=config['visualization']['s']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "In this notebook, we implemented an unsupervised learning pipeline for customer segmentation in an e-commerce scenario. The pipeline includes:\n",
    "\n",
    "1. **Data Preprocessing**: We handled missing values, removed outliers, scaled features, and encoded categorical variables.\n",
    "\n",
    "2. **Dimensionality Reduction**: We compared PCA, Kernel PCA, MDS, and UMAP for reducing the dimensionality of the data.\n",
    "\n",
    "3. **Clustering**: We applied KMeans, DBSCAN, and Agglomerative Clustering to identify distinct customer groups.\n",
    "\n",
    "4. **Evaluation**: We evaluated the clustering results using silhouette score, Davies-Bouldin index, and visual inspection.\n",
    "\n",
    "5. **Interpretation**: We analyzed the cluster profiles and generated human-readable labels for each cluster.\n",
    "\n",
    "6. **Robustness**: We tested the pipeline on small datasets, high-dimensional noisy data, and imbalanced clusters.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- The optimal number of clusters for this dataset was determined to be [optimal_k_silhouette].\n",
    "- The best performing clustering method was [best_method] based on the silhouette score.\n",
    "- We identified distinct customer segments with the following characteristics: [list cluster labels].\n",
    "- The pipeline demonstrated robustness to various edge cases, including small datasets, noisy data, and imbalanced clusters.\n",
    "\n",
    "### Business Applications\n",
    "\n",
    "These customer segments can be used for:\n",
    "\n",
    "1. **Targeted Marketing**: Tailoring marketing campaigns to specific customer segments.\n",
    "2. **Personalization**: Customizing the user experience based on the segment a customer belongs to.\n",
    "3. **Customer Retention**: Developing strategies to retain customers in high-value segments.\n",
    "4. **Product Recommendations**: Recommending products based on the preferences of similar customers in the same segment.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "1. **Feature Engineering**: Develop more sophisticated features to better capture customer behavior.\n",
    "2. **Time-Series Analysis**: Incorporate temporal patterns in customer behavior.\n",
    "3. **Semi-Supervised Learning**: Use labeled data to guide the clustering process.\n",
    "4. **Interactive Dashboard**: Build a Streamlit or Dash app for interactive exploration of customer segments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}